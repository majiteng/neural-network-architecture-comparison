{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711c7820-304d-445d-b723-233f025fdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Flatten\n",
    "import time \n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, SAGEConv, GATConv, SplineConv, Linear, to_hetero\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset, download_url, HeteroData\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "from d2l import torch as d2l\n",
    "from DoubleBoxDataLoader import HeteroTrainData, HeteroTestData\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_train = HeteroTrainData(root = 'DoubleBox_Branchline_data/train/')\n",
    "dataset_test = HeteroTestData(root = 'DoubleBox_Branchline_data/test/')\n",
    "# dataset_train = dataset_train[0:1200]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "# for data in train_loader:\n",
    "#     data = data\n",
    "# #     print(data)\n",
    "#     break\n",
    "data = dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043e47a1-2b26-4718-be4c-70e1170c6bf8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Trainloss: 0.896187, TestR2: -0.3236\n",
      "Epoch: 02, Trainloss: 0.193142, TestR2: -0.4886\n",
      "Epoch: 03, Trainloss: 0.183711, TestR2: -0.3022\n",
      "Epoch: 04, Trainloss: 0.167672, TestR2: -0.2151\n",
      "Epoch: 05, Trainloss: 0.146087, TestR2: -0.1698\n",
      "Epoch: 06, Trainloss: 0.113474, TestR2: -0.1677\n",
      "Epoch: 07, Trainloss: 0.080839, TestR2: -0.1735\n",
      "Epoch: 08, Trainloss: 0.057150, TestR2: -0.1658\n",
      "Epoch: 09, Trainloss: 0.044224, TestR2: -0.1118\n",
      "Epoch: 10, Trainloss: 0.037142, TestR2: -0.0847\n",
      "Epoch: 11, Trainloss: 0.034565, TestR2: -0.0642\n",
      "Epoch: 12, Trainloss: 0.034308, TestR2: -0.0073\n",
      "Epoch: 13, Trainloss: 0.032989, TestR2: 0.0157\n",
      "Epoch: 14, Trainloss: 0.030893, TestR2: -0.0143\n",
      "Epoch: 15, Trainloss: 0.031684, TestR2: 0.0452\n",
      "Epoch: 16, Trainloss: 0.029462, TestR2: 0.0448\n",
      "Epoch: 17, Trainloss: 0.029462, TestR2: 0.0805\n",
      "Epoch: 18, Trainloss: 0.028359, TestR2: 0.0908\n",
      "Epoch: 19, Trainloss: 0.027612, TestR2: 0.1094\n",
      "Epoch: 20, Trainloss: 0.025387, TestR2: 0.1205\n",
      "Epoch: 21, Trainloss: 0.024178, TestR2: 0.1316\n",
      "Epoch: 22, Trainloss: 0.021405, TestR2: 0.1420\n",
      "Epoch: 23, Trainloss: 0.018607, TestR2: 0.1245\n",
      "Epoch: 24, Trainloss: 0.016766, TestR2: 0.1472\n",
      "Epoch: 25, Trainloss: 0.015159, TestR2: 0.1109\n",
      "Epoch: 26, Trainloss: 0.014769, TestR2: 0.1363\n",
      "Epoch: 27, Trainloss: 0.015289, TestR2: 0.1905\n",
      "Epoch: 28, Trainloss: 0.014397, TestR2: 0.2778\n",
      "Epoch: 29, Trainloss: 0.013151, TestR2: 0.3674\n",
      "Epoch: 30, Trainloss: 0.011837, TestR2: 0.4266\n",
      "Epoch: 31, Trainloss: 0.010456, TestR2: 0.4687\n",
      "Epoch: 32, Trainloss: 0.009608, TestR2: 0.4918\n",
      "Epoch: 33, Trainloss: 0.008874, TestR2: 0.5073\n",
      "Epoch: 34, Trainloss: 0.008371, TestR2: 0.5187\n",
      "Epoch: 35, Trainloss: 0.007977, TestR2: 0.5273\n",
      "Epoch: 36, Trainloss: 0.007660, TestR2: 0.5360\n",
      "Epoch: 37, Trainloss: 0.007384, TestR2: 0.5446\n",
      "Epoch: 38, Trainloss: 0.007137, TestR2: 0.5511\n",
      "Epoch: 39, Trainloss: 0.006916, TestR2: 0.5588\n",
      "Epoch: 40, Trainloss: 0.006725, TestR2: 0.5659\n",
      "Epoch: 41, Trainloss: 0.006566, TestR2: 0.5735\n",
      "Epoch: 42, Trainloss: 0.006414, TestR2: 0.5801\n",
      "Epoch: 43, Trainloss: 0.006222, TestR2: 0.5876\n",
      "Epoch: 44, Trainloss: 0.006069, TestR2: 0.5939\n",
      "Epoch: 45, Trainloss: 0.005928, TestR2: 0.5997\n",
      "Epoch: 46, Trainloss: 0.005801, TestR2: 0.6059\n",
      "Epoch: 47, Trainloss: 0.005673, TestR2: 0.6117\n",
      "Epoch: 48, Trainloss: 0.005547, TestR2: 0.6173\n",
      "Epoch: 49, Trainloss: 0.005430, TestR2: 0.6225\n",
      "Epoch: 50, Trainloss: 0.005322, TestR2: 0.6271\n",
      "Epoch: 51, Trainloss: 0.005218, TestR2: 0.6316\n",
      "Epoch: 52, Trainloss: 0.005123, TestR2: 0.6357\n",
      "Epoch: 53, Trainloss: 0.005025, TestR2: 0.6393\n",
      "Epoch: 54, Trainloss: 0.004946, TestR2: 0.6427\n",
      "Epoch: 55, Trainloss: 0.004865, TestR2: 0.6456\n",
      "Epoch: 56, Trainloss: 0.004784, TestR2: 0.6485\n",
      "Epoch: 57, Trainloss: 0.004711, TestR2: 0.6513\n",
      "Epoch: 58, Trainloss: 0.004637, TestR2: 0.6536\n",
      "Epoch: 59, Trainloss: 0.004567, TestR2: 0.6558\n",
      "Epoch: 60, Trainloss: 0.004495, TestR2: 0.6580\n",
      "Epoch: 61, Trainloss: 0.004438, TestR2: 0.6597\n",
      "Epoch: 62, Trainloss: 0.004379, TestR2: 0.6615\n",
      "Epoch: 63, Trainloss: 0.004326, TestR2: 0.6633\n",
      "Epoch: 64, Trainloss: 0.004272, TestR2: 0.6650\n",
      "Epoch: 65, Trainloss: 0.004221, TestR2: 0.6662\n",
      "Epoch: 66, Trainloss: 0.004174, TestR2: 0.6672\n",
      "Epoch: 67, Trainloss: 0.004134, TestR2: 0.6687\n",
      "Epoch: 68, Trainloss: 0.004091, TestR2: 0.6702\n",
      "Epoch: 69, Trainloss: 0.004066, TestR2: 0.6721\n",
      "Epoch: 70, Trainloss: 0.004048, TestR2: 0.6739\n",
      "Epoch: 71, Trainloss: 0.004045, TestR2: 0.6758\n",
      "Epoch: 72, Trainloss: 0.004066, TestR2: 0.6779\n",
      "Epoch: 73, Trainloss: 0.004121, TestR2: 0.6802\n",
      "Epoch: 74, Trainloss: 0.004190, TestR2: 0.6826\n",
      "Epoch: 75, Trainloss: 0.004205, TestR2: 0.6851\n",
      "Epoch: 76, Trainloss: 0.004129, TestR2: 0.6879\n",
      "Epoch: 77, Trainloss: 0.004003, TestR2: 0.6907\n",
      "Epoch: 78, Trainloss: 0.003878, TestR2: 0.6929\n",
      "Epoch: 79, Trainloss: 0.003777, TestR2: 0.6947\n",
      "Epoch: 80, Trainloss: 0.003706, TestR2: 0.6958\n",
      "Epoch: 81, Trainloss: 0.003647, TestR2: 0.6965\n",
      "Epoch: 82, Trainloss: 0.003586, TestR2: 0.6968\n",
      "Epoch: 83, Trainloss: 0.003547, TestR2: 0.6970\n",
      "Epoch: 84, Trainloss: 0.003507, TestR2: 0.6973\n",
      "Epoch: 85, Trainloss: 0.003470, TestR2: 0.6976\n",
      "Epoch: 86, Trainloss: 0.003438, TestR2: 0.6980\n",
      "Epoch: 87, Trainloss: 0.003410, TestR2: 0.6991\n",
      "Epoch: 88, Trainloss: 0.003384, TestR2: 0.7000\n",
      "Epoch: 89, Trainloss: 0.003358, TestR2: 0.7018\n",
      "Epoch: 90, Trainloss: 0.003331, TestR2: 0.7036\n",
      "Epoch: 91, Trainloss: 0.003306, TestR2: 0.7056\n",
      "Epoch: 92, Trainloss: 0.003281, TestR2: 0.7068\n",
      "Epoch: 93, Trainloss: 0.003256, TestR2: 0.7089\n",
      "Epoch: 94, Trainloss: 0.003226, TestR2: 0.7110\n",
      "Epoch: 95, Trainloss: 0.003192, TestR2: 0.7139\n",
      "Epoch: 96, Trainloss: 0.003172, TestR2: 0.7158\n",
      "Epoch: 97, Trainloss: 0.003147, TestR2: 0.7196\n",
      "Epoch: 98, Trainloss: 0.003118, TestR2: 0.7216\n",
      "Epoch: 99, Trainloss: 0.003090, TestR2: 0.7236\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.lin1 = Linear(8, 8)\n",
    "        self.conv1 = SAGEConv(16, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "        return x\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ML_lin = torch.nn.Linear(2,16)\n",
    "        self.MT_lin = torch.nn.Linear(3,16)\n",
    "        self.gcn = GCN()\n",
    "        self.gcn = to_hetero(self.gcn, metadata=data.metadata())    \n",
    "        self.flat = Flatten()\n",
    "        self.lin1 = Linear(-1, 128)\n",
    "        self.lin2 = Linear(128, 256)\n",
    "        self.lin3 = Linear(256, 40)\n",
    "        self.dout1 = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        \n",
    "        x_dict = {\n",
    "            \"ML\": self.ML_lin(data[\"ML\"].x),\n",
    "            \"MT\": self.MT_lin(data[\"MT\"].x),\n",
    "        }\n",
    "        x = self.gcn(x_dict, data.edge_index_dict)\n",
    "#         x = global_mean_pool(x[\"ML\"], 1000)\n",
    "        x[\"ML\"] = x[\"ML\"].reshape(batch_size,-1)\n",
    "        x[\"MT\"] = x[\"MT\"].reshape(batch_size,-1)\n",
    "#         print(self.flat(x[\"MT\"]).shape)\n",
    "        x = torch.cat((x[\"ML\"],x[\"MT\"]),1)   #把两个向量直接连起来\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.dout1(x)\n",
    "        x = self.lin2(x).relu()\n",
    "#         x = global_mean_pool(x, batch)\n",
    "        out = self.lin3(x)\n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "mse_loss = F.mse_loss\n",
    "# model.load_state_dict(torch.load('GCN_Forward1.params'))\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "#     ys, preds = [], []\n",
    "    for data in train_loader:\n",
    "#         print(data.y.shape)\n",
    "        data = data.to(device)\n",
    "#         ys.append(data['ML'].y.reshape(batch_size,-1))\n",
    "        optimizer.zero_grad()\n",
    "#         testout = torch.cat((x[:,0], x[:,1]), 0)\n",
    "        out = model(data)\n",
    "#         preds.append(out.cpu())\n",
    "#         print(out.shape)\n",
    "#         print(torch.reshape(data.y, (100,8)))\n",
    "        loss = mse_loss(out, data['ML'].y.reshape(batch_size,-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    return loss\n",
    "#     y, pred = torch.cat(ys, dim=0).cpu().detach().numpy(), torch.cat(preds, dim=0).detach().numpy()\n",
    "    \n",
    "#     return r2_score(y, pred)\n",
    "\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        ys.append(data['ML'].y.reshape(batch_size,-1))\n",
    "        out = model(data)\n",
    "        preds.append(out)\n",
    "        \n",
    "    y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "    loss = r2_score(y.to(\"cpu\").detach().numpy(), pred.to(\"cpu\").detach().numpy())\n",
    "#     print(pred)\n",
    "#     print(y)\n",
    "    return loss, pred, y\n",
    "\n",
    "R2_accuracy = []\n",
    "time1 = time.time()\n",
    "for epoch in range(1, 100):\n",
    "    train_loss = train(train_loader)\n",
    "    test_f1, pred, y = test(test_loader)\n",
    "    R2_accuracy.append(test_f1)\n",
    "    print(f'Epoch: {epoch:02d}, Trainloss: {train_loss:.6f}, TestR2: {test_f1:.4f}')\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a95ac9-4712-46dd-85e6-5234f8210e39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Trainloss: 0.002355, TestR2: 0.7741\n",
      "Epoch: 02, Trainloss: 0.002365, TestR2: 0.7733\n",
      "Epoch: 03, Trainloss: 0.002376, TestR2: 0.7758\n",
      "Epoch: 04, Trainloss: 0.002402, TestR2: 0.7804\n",
      "Epoch: 05, Trainloss: 0.002443, TestR2: 0.7847\n",
      "Epoch: 06, Trainloss: 0.002356, TestR2: 0.7877\n",
      "Epoch: 07, Trainloss: 0.002202, TestR2: 0.7894\n",
      "Epoch: 08, Trainloss: 0.002150, TestR2: 0.7901\n",
      "Epoch: 09, Trainloss: 0.002134, TestR2: 0.7906\n",
      "Epoch: 10, Trainloss: 0.002126, TestR2: 0.7905\n",
      "Epoch: 11, Trainloss: 0.002120, TestR2: 0.7902\n",
      "Epoch: 12, Trainloss: 0.002118, TestR2: 0.7899\n",
      "Epoch: 13, Trainloss: 0.002116, TestR2: 0.7895\n",
      "Epoch: 14, Trainloss: 0.002118, TestR2: 0.7892\n",
      "Epoch: 15, Trainloss: 0.002125, TestR2: 0.7891\n",
      "Epoch: 16, Trainloss: 0.002134, TestR2: 0.7892\n",
      "Epoch: 17, Trainloss: 0.002145, TestR2: 0.7896\n",
      "Epoch: 18, Trainloss: 0.002153, TestR2: 0.7903\n",
      "Epoch: 19, Trainloss: 0.002154, TestR2: 0.7910\n",
      "Epoch: 20, Trainloss: 0.002154, TestR2: 0.7918\n",
      "Epoch: 21, Trainloss: 0.002153, TestR2: 0.7927\n",
      "Epoch: 22, Trainloss: 0.002141, TestR2: 0.7934\n",
      "Epoch: 23, Trainloss: 0.002120, TestR2: 0.7941\n",
      "Epoch: 24, Trainloss: 0.002098, TestR2: 0.7945\n",
      "Epoch: 25, Trainloss: 0.002091, TestR2: 0.7945\n",
      "Epoch: 26, Trainloss: 0.002102, TestR2: 0.7940\n",
      "Epoch: 27, Trainloss: 0.002129, TestR2: 0.7933\n",
      "Epoch: 28, Trainloss: 0.002158, TestR2: 0.7927\n",
      "Epoch: 29, Trainloss: 0.002171, TestR2: 0.7929\n",
      "Epoch: 30, Trainloss: 0.002154, TestR2: 0.7936\n",
      "Epoch: 31, Trainloss: 0.002117, TestR2: 0.7943\n",
      "Epoch: 32, Trainloss: 0.002082, TestR2: 0.7946\n",
      "Epoch: 33, Trainloss: 0.002060, TestR2: 0.7943\n",
      "Epoch: 34, Trainloss: 0.002053, TestR2: 0.7938\n",
      "Epoch: 35, Trainloss: 0.002052, TestR2: 0.7933\n",
      "Epoch: 36, Trainloss: 0.002058, TestR2: 0.7932\n",
      "Epoch: 37, Trainloss: 0.002059, TestR2: 0.7935\n",
      "Epoch: 38, Trainloss: 0.002047, TestR2: 0.7947\n",
      "Epoch: 39, Trainloss: 0.002015, TestR2: 0.7964\n",
      "Epoch: 40, Trainloss: 0.001979, TestR2: 0.7982\n",
      "Epoch: 41, Trainloss: 0.001951, TestR2: 0.7992\n",
      "Epoch: 42, Trainloss: 0.001936, TestR2: 0.7996\n",
      "Epoch: 43, Trainloss: 0.001933, TestR2: 0.7998\n",
      "Epoch: 44, Trainloss: 0.001939, TestR2: 0.7995\n",
      "Epoch: 45, Trainloss: 0.001952, TestR2: 0.7992\n",
      "Epoch: 46, Trainloss: 0.001971, TestR2: 0.7994\n",
      "Epoch: 47, Trainloss: 0.001990, TestR2: 0.8003\n",
      "Epoch: 48, Trainloss: 0.001987, TestR2: 0.8018\n",
      "Epoch: 49, Trainloss: 0.001955, TestR2: 0.8034\n",
      "Epoch: 50, Trainloss: 0.001915, TestR2: 0.8050\n",
      "Epoch: 51, Trainloss: 0.001891, TestR2: 0.8062\n",
      "Epoch: 52, Trainloss: 0.001881, TestR2: 0.8069\n",
      "Epoch: 53, Trainloss: 0.001882, TestR2: 0.8070\n",
      "Epoch: 54, Trainloss: 0.001898, TestR2: 0.8068\n",
      "Epoch: 55, Trainloss: 0.001933, TestR2: 0.8065\n",
      "Epoch: 56, Trainloss: 0.001990, TestR2: 0.8058\n",
      "Epoch: 57, Trainloss: 0.002027, TestR2: 0.8051\n",
      "Epoch: 58, Trainloss: 0.001957, TestR2: 0.8047\n",
      "Epoch: 59, Trainloss: 0.001873, TestR2: 0.8042\n",
      "Epoch: 60, Trainloss: 0.001841, TestR2: 0.8036\n",
      "Epoch: 61, Trainloss: 0.001838, TestR2: 0.8028\n",
      "Epoch: 62, Trainloss: 0.001846, TestR2: 0.8022\n",
      "Epoch: 63, Trainloss: 0.001850, TestR2: 0.8019\n",
      "Epoch: 64, Trainloss: 0.001854, TestR2: 0.8021\n",
      "Epoch: 65, Trainloss: 0.001852, TestR2: 0.8026\n",
      "Epoch: 66, Trainloss: 0.001844, TestR2: 0.8036\n",
      "Epoch: 67, Trainloss: 0.001833, TestR2: 0.8049\n",
      "Epoch: 68, Trainloss: 0.001823, TestR2: 0.8062\n",
      "Epoch: 69, Trainloss: 0.001818, TestR2: 0.8076\n",
      "Epoch: 70, Trainloss: 0.001822, TestR2: 0.8088\n",
      "Epoch: 71, Trainloss: 0.001834, TestR2: 0.8101\n",
      "Epoch: 72, Trainloss: 0.001859, TestR2: 0.8111\n",
      "Epoch: 73, Trainloss: 0.001882, TestR2: 0.8122\n",
      "Epoch: 74, Trainloss: 0.001863, TestR2: 0.8132\n",
      "Epoch: 75, Trainloss: 0.001805, TestR2: 0.8133\n",
      "Epoch: 76, Trainloss: 0.001758, TestR2: 0.8129\n",
      "Epoch: 77, Trainloss: 0.001740, TestR2: 0.8121\n",
      "Epoch: 78, Trainloss: 0.001746, TestR2: 0.8107\n",
      "Epoch: 79, Trainloss: 0.001763, TestR2: 0.8093\n",
      "Epoch: 80, Trainloss: 0.001784, TestR2: 0.8087\n",
      "Epoch: 81, Trainloss: 0.001801, TestR2: 0.8090\n",
      "Epoch: 82, Trainloss: 0.001802, TestR2: 0.8103\n",
      "Epoch: 83, Trainloss: 0.001795, TestR2: 0.8115\n",
      "Epoch: 84, Trainloss: 0.001788, TestR2: 0.8122\n",
      "Epoch: 85, Trainloss: 0.001789, TestR2: 0.8118\n",
      "Epoch: 86, Trainloss: 0.001800, TestR2: 0.8115\n",
      "Epoch: 87, Trainloss: 0.001818, TestR2: 0.8113\n",
      "Epoch: 88, Trainloss: 0.001833, TestR2: 0.8122\n",
      "Epoch: 89, Trainloss: 0.001845, TestR2: 0.8140\n",
      "Epoch: 90, Trainloss: 0.001852, TestR2: 0.8158\n",
      "Epoch: 91, Trainloss: 0.001840, TestR2: 0.8173\n",
      "Epoch: 92, Trainloss: 0.001787, TestR2: 0.8181\n",
      "Epoch: 93, Trainloss: 0.001725, TestR2: 0.8183\n",
      "Epoch: 94, Trainloss: 0.001692, TestR2: 0.8178\n",
      "Epoch: 95, Trainloss: 0.001685, TestR2: 0.8169\n",
      "Epoch: 96, Trainloss: 0.001693, TestR2: 0.8160\n",
      "Epoch: 97, Trainloss: 0.001712, TestR2: 0.8150\n",
      "Epoch: 98, Trainloss: 0.001735, TestR2: 0.8142\n",
      "Epoch: 99, Trainloss: 0.001761, TestR2: 0.8131\n",
      "Epoch: 100, Trainloss: 0.001772, TestR2: 0.8121\n",
      "Epoch: 101, Trainloss: 0.001764, TestR2: 0.8109\n",
      "Epoch: 102, Trainloss: 0.001740, TestR2: 0.8085\n",
      "Epoch: 103, Trainloss: 0.001718, TestR2: 0.8047\n",
      "Epoch: 104, Trainloss: 0.001714, TestR2: 0.7987\n",
      "Epoch: 105, Trainloss: 0.001727, TestR2: 0.7916\n",
      "Epoch: 106, Trainloss: 0.001761, TestR2: 0.7875\n",
      "Epoch: 107, Trainloss: 0.001792, TestR2: 0.7916\n",
      "Epoch: 108, Trainloss: 0.001764, TestR2: 0.8013\n",
      "Epoch: 109, Trainloss: 0.001675, TestR2: 0.8115\n",
      "Epoch: 110, Trainloss: 0.001601, TestR2: 0.8182\n",
      "Epoch: 111, Trainloss: 0.001565, TestR2: 0.8219\n",
      "Epoch: 112, Trainloss: 0.001550, TestR2: 0.8239\n",
      "Epoch: 113, Trainloss: 0.001545, TestR2: 0.8249\n",
      "Epoch: 114, Trainloss: 0.001548, TestR2: 0.8254\n",
      "Epoch: 115, Trainloss: 0.001555, TestR2: 0.8252\n",
      "Epoch: 116, Trainloss: 0.001571, TestR2: 0.8246\n",
      "Epoch: 117, Trainloss: 0.001599, TestR2: 0.8235\n",
      "Epoch: 118, Trainloss: 0.001639, TestR2: 0.8223\n",
      "Epoch: 119, Trainloss: 0.001663, TestR2: 0.8211\n",
      "Epoch: 120, Trainloss: 0.001643, TestR2: 0.8206\n",
      "Epoch: 121, Trainloss: 0.001618, TestR2: 0.8211\n",
      "Epoch: 122, Trainloss: 0.001611, TestR2: 0.8219\n",
      "Epoch: 123, Trainloss: 0.001635, TestR2: 0.8224\n",
      "Epoch: 124, Trainloss: 0.001729, TestR2: 0.8225\n",
      "Epoch: 125, Trainloss: 0.001898, TestR2: 0.8223\n",
      "Epoch: 126, Trainloss: 0.001735, TestR2: 0.8243\n",
      "Epoch: 127, Trainloss: 0.001553, TestR2: 0.8269\n",
      "Epoch: 128, Trainloss: 0.001516, TestR2: 0.8276\n",
      "Epoch: 129, Trainloss: 0.001503, TestR2: 0.8274\n",
      "Epoch: 130, Trainloss: 0.001498, TestR2: 0.8268\n",
      "Epoch: 131, Trainloss: 0.001496, TestR2: 0.8258\n",
      "Epoch: 132, Trainloss: 0.001502, TestR2: 0.8242\n",
      "Epoch: 133, Trainloss: 0.001513, TestR2: 0.8224\n",
      "Epoch: 134, Trainloss: 0.001531, TestR2: 0.8205\n",
      "Epoch: 135, Trainloss: 0.001561, TestR2: 0.8188\n",
      "Epoch: 136, Trainloss: 0.001593, TestR2: 0.8179\n",
      "Epoch: 137, Trainloss: 0.001612, TestR2: 0.8187\n",
      "Epoch: 138, Trainloss: 0.001610, TestR2: 0.8205\n",
      "Epoch: 139, Trainloss: 0.001586, TestR2: 0.8222\n",
      "Epoch: 140, Trainloss: 0.001560, TestR2: 0.8232\n",
      "Epoch: 141, Trainloss: 0.001544, TestR2: 0.8233\n",
      "Epoch: 142, Trainloss: 0.001547, TestR2: 0.8228\n",
      "Epoch: 143, Trainloss: 0.001583, TestR2: 0.8219\n",
      "Epoch: 144, Trainloss: 0.001655, TestR2: 0.8213\n",
      "Epoch: 145, Trainloss: 0.001655, TestR2: 0.8213\n",
      "Epoch: 146, Trainloss: 0.001547, TestR2: 0.8214\n",
      "Epoch: 147, Trainloss: 0.001522, TestR2: 0.8221\n",
      "Epoch: 148, Trainloss: 0.001505, TestR2: 0.8243\n",
      "Epoch: 149, Trainloss: 0.001486, TestR2: 0.8263\n",
      "Epoch: 150, Trainloss: 0.001471, TestR2: 0.8277\n",
      "Epoch: 151, Trainloss: 0.001463, TestR2: 0.8286\n",
      "Epoch: 152, Trainloss: 0.001463, TestR2: 0.8290\n",
      "Epoch: 153, Trainloss: 0.001467, TestR2: 0.8292\n",
      "Epoch: 154, Trainloss: 0.001476, TestR2: 0.8292\n",
      "Epoch: 155, Trainloss: 0.001488, TestR2: 0.8291\n",
      "Epoch: 156, Trainloss: 0.001498, TestR2: 0.8296\n",
      "Epoch: 157, Trainloss: 0.001503, TestR2: 0.8302\n",
      "Epoch: 158, Trainloss: 0.001500, TestR2: 0.8312\n",
      "Epoch: 159, Trainloss: 0.001488, TestR2: 0.8321\n",
      "Epoch: 160, Trainloss: 0.001469, TestR2: 0.8330\n",
      "Epoch: 161, Trainloss: 0.001451, TestR2: 0.8335\n",
      "Epoch: 162, Trainloss: 0.001441, TestR2: 0.8336\n",
      "Epoch: 163, Trainloss: 0.001438, TestR2: 0.8334\n",
      "Epoch: 164, Trainloss: 0.001439, TestR2: 0.8329\n",
      "Epoch: 165, Trainloss: 0.001444, TestR2: 0.8322\n",
      "Epoch: 166, Trainloss: 0.001457, TestR2: 0.8311\n",
      "Epoch: 167, Trainloss: 0.001477, TestR2: 0.8295\n",
      "Epoch: 168, Trainloss: 0.001511, TestR2: 0.8274\n",
      "Epoch: 169, Trainloss: 0.001566, TestR2: 0.8257\n",
      "Epoch: 170, Trainloss: 0.001741, TestR2: 0.8267\n",
      "Epoch: 171, Trainloss: 0.001800, TestR2: 0.8303\n",
      "Epoch: 172, Trainloss: 0.001543, TestR2: 0.8343\n",
      "Epoch: 173, Trainloss: 0.001434, TestR2: 0.8355\n",
      "Epoch: 174, Trainloss: 0.001407, TestR2: 0.8359\n",
      "Epoch: 175, Trainloss: 0.001397, TestR2: 0.8354\n",
      "Epoch: 176, Trainloss: 0.001389, TestR2: 0.8349\n",
      "Epoch: 177, Trainloss: 0.001384, TestR2: 0.8344\n",
      "Epoch: 178, Trainloss: 0.001382, TestR2: 0.8339\n",
      "Epoch: 179, Trainloss: 0.001384, TestR2: 0.8333\n",
      "Epoch: 180, Trainloss: 0.001388, TestR2: 0.8325\n",
      "Epoch: 181, Trainloss: 0.001395, TestR2: 0.8317\n",
      "Epoch: 182, Trainloss: 0.001404, TestR2: 0.8311\n",
      "Epoch: 183, Trainloss: 0.001413, TestR2: 0.8307\n",
      "Epoch: 184, Trainloss: 0.001426, TestR2: 0.8304\n",
      "Epoch: 185, Trainloss: 0.001439, TestR2: 0.8308\n",
      "Epoch: 186, Trainloss: 0.001461, TestR2: 0.8312\n",
      "Epoch: 187, Trainloss: 0.001498, TestR2: 0.8317\n",
      "Epoch: 188, Trainloss: 0.001544, TestR2: 0.8314\n",
      "Epoch: 189, Trainloss: 0.001530, TestR2: 0.8303\n",
      "Epoch: 190, Trainloss: 0.001444, TestR2: 0.8287\n",
      "Epoch: 191, Trainloss: 0.001387, TestR2: 0.8278\n",
      "Epoch: 192, Trainloss: 0.001365, TestR2: 0.8275\n",
      "Epoch: 193, Trainloss: 0.001366, TestR2: 0.8262\n",
      "Epoch: 194, Trainloss: 0.001383, TestR2: 0.8248\n",
      "Epoch: 195, Trainloss: 0.001409, TestR2: 0.8240\n",
      "Epoch: 196, Trainloss: 0.001435, TestR2: 0.8258\n",
      "Epoch: 197, Trainloss: 0.001436, TestR2: 0.8306\n",
      "Epoch: 198, Trainloss: 0.001412, TestR2: 0.8355\n",
      "Epoch: 199, Trainloss: 0.001385, TestR2: 0.8381\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200):\n",
    "    train_loss = train(train_loader)\n",
    "    test_f1, pred, y = test(test_loader)\n",
    "    R2_accuracy.append(test_f1)\n",
    "    print(f'Epoch: {epoch:02d}, Trainloss: {train_loss:.6f}, TestR2: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d8074-8639-4aee-99ce-0152dfd883d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training time:\", round(time2-time1, 2), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ffcb2-eee3-4051-8df2-debbab91f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelSize(model):\n",
    "    param_size = 0\n",
    "    param_sum = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        param_sum += param.nelement()\n",
    "    buffer_size = 0\n",
    "    buffer_sum = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        buffer_sum += buffer.nelement()\n",
    "    all_size = (param_size + buffer_size) / 1024\n",
    "    print('The model size is：{:.3f}KB'.format(all_size))\n",
    "    return (param_size, param_sum, buffer_size, buffer_sum, all_size)\n",
    "    \n",
    "a, b, c, d, e = getModelSize(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
